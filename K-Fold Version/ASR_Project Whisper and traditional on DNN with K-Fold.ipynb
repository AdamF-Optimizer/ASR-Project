{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ! pip install librosa\n",
        "# ! pip install parselmouth\n",
        "# ! pip install pyAudioAnalysis\n",
        "# ! pip install torch\n",
        "# ! pip install transformers\n",
        "# ! pip install pandas\n",
        "# ! pip install tqdm\n",
        "# ! pip install scikit-learn\n",
        "# ! pip install parselmouth\n",
        "# ! pip install nolds"
      ],
      "metadata": {
        "id": "BXdHuMT7K5or"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Project"
      ],
      "metadata": {
        "id": "fni6FlZtKC5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0868eb8-a556-497c-fafd-575dc41a3953"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "import librosa\n",
        "import parselmouth\n",
        "from parselmouth.praat import call\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score, precision_score, roc_auc_score, roc_curve, auc\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoFeatureExtractor, WhisperModel, AutoModelForAudioClassification\n",
        "import logging\n",
        "import gc\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nolds\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "aa0bO-l17EdV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"ParkCeleb_filtered\"\n",
        "groups = {\"PD\": 1, \"CN\": 0}\n",
        "\n",
        "parkinson_files = []\n",
        "control_files = []\n",
        "\n",
        "# Loop over each group (PD, CN)\n",
        "for group, label in groups.items():\n",
        "    group_path = os.path.join(base_path, group)\n",
        "    # Recursively search for any .wav file\n",
        "    wav_files = glob(os.path.join(group_path, \"**\", \"*.wav\"), recursive=True)\n",
        "\n",
        "    if label == 1:\n",
        "        parkinson_files.extend(wav_files)\n",
        "    else:\n",
        "        control_files.extend(wav_files)\n",
        "\n",
        "# Merge and create labels\n",
        "all_files = parkinson_files + control_files\n",
        "labels = [1] * len(parkinson_files) + [0] * len(control_files)\n",
        "\n",
        "print(f\"Found {len(parkinson_files)} Parkinson files and {len(control_files)} Control files.\")\n"
      ],
      "metadata": {
        "id": "s-VfFLIIKwYQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46adbd29-67c9-4d8f-be66-c22b4fed096a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8708 Parkinson files and 5349 Control files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load ParkCeleb dataset and extract speaker IDs"
      ],
      "metadata": {
        "id": "VboBLoTI-Rlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_parkceleb_dataset_with_speakers(dataset_path):\n",
        "    groups = {\"PD\": 1, \"CN\": 0}\n",
        "    audio_files = []\n",
        "    labels = []\n",
        "    speaker_ids = []\n",
        "\n",
        "    # Loop over each group (PD, CN)\n",
        "    for group, label in groups.items():\n",
        "        group_path = os.path.join(dataset_path, group)\n",
        "        # Recursively search for the specific file\n",
        "        for audio_file in glob(os.path.join(group_path, \"**\", \"*.wav\"), recursive=True):\n",
        "            audio_files.append(audio_file)\n",
        "            labels.append(label)\n",
        "\n",
        "            # Extract speaker ID from the path (assuming structure where speaker ID is the directory name after the group name e.g. CN/cn_01 or PD/pd_01)\n",
        "            try:\n",
        "                # Example: dataset_path/PD/speaker01/utterance_1.wav\n",
        "                relative_path = os.path.relpath(audio_file, group_path)\n",
        "                speaker_id = relative_path.split(os.sep)[0]\n",
        "                speaker_ids.append(speaker_id)\n",
        "            except IndexError:\n",
        "                print(f\"Could not extract speaker ID from path: {audio_file}\")\n",
        "                speaker_ids.append(None) # Handle cases where speaker ID cannot be extracted, caused by errors in dataset\n",
        "\n",
        "    # Filter out files where speaker ID couldn't be determined, some stupid bug\n",
        "    valid_indices = [i for i, spkr_id in enumerate(speaker_ids) if spkr_id is not None]\n",
        "    audio_files = [audio_files[i] for i in valid_indices]\n",
        "    labels = [labels[i] for i in valid_indices]\n",
        "    speaker_ids = [speaker_ids[i] for i in valid_indices]\n",
        "\n",
        "    return audio_files, labels, speaker_ids"
      ],
      "metadata": {
        "id": "_I5raZBx-R2X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create whisper features"
      ],
      "metadata": {
        "id": "aFbQrRZpC35N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_whisper_features(audio_paths, model_name=\"openai/whisper-small\", min_time=0.1):\n",
        "    model = WhisperModel.from_pretrained(model_name)\n",
        "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    features = []\n",
        "    labels = []\n",
        "    ids = []\n",
        "    times = []\n",
        "    lengths = []\n",
        "    successful_paths = []\n",
        "\n",
        "    for audio_path in tqdm(audio_paths, desc=\"Extracting Whisper features\"):\n",
        "        try:\n",
        "            audio_array, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
        "            # print(audio_path)\n",
        "\n",
        "            # Extract features using the feature extractor\n",
        "            inputs = feature_extractor(\n",
        "                audio_array,\n",
        "                sampling_rate=16000,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            # Define a decoder input for Whisper (needed for the forward pass structure)\n",
        "            # This is not really used, but whisper requires it to be passed, regardless if you do or do not use the decoder.\n",
        "            decoder_input_ids = torch.tensor([[1] * 100]).to(device) # Add 100 tokens for the decoder input\n",
        "\n",
        "            # Move inputs to the correct device\n",
        "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "            # Forward pass to get hidden states\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n",
        "\n",
        "            # Get the last hidden state from the encoder and average across the sequence dimension\n",
        "            embeddings = outputs.encoder_hidden_states[-1].mean(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "\n",
        "            # get patient id (pd_01 etc)\n",
        "            patient_id = audio_path.split(\"/\")[2]\n",
        "\n",
        "            # Get time before diagnosis\n",
        "            speaker_dir = Path(audio_path).parent\n",
        "            audio_dir = Path(speaker_dir).parent\n",
        "            time_before_diagnosis = get_time_before_diagnosis(audio_dir)\n",
        "\n",
        "            group = Path(audio_path).parent.parent.name  # Parent directory of speaker dir\n",
        "            label = 1 if group == \"PD\" else 0  # Directly use group name\n",
        "\n",
        "            features.append(embeddings)\n",
        "            successful_paths.append(audio_path)\n",
        "            labels.append(label)  # Use corrected label\n",
        "            ids.append(patient_id)\n",
        "            times.append(time_before_diagnosis)\n",
        "            lengths.append(librosa.get_duration(y=audio_array, sr=sr))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to process {audio_path}: {e}\")\n",
        "\n",
        "    if not features:\n",
        "        return np.array([]), [] # Return empty arrays if no features were extracted\n",
        "\n",
        "    return np.array(features), list(labels), list(ids), list(times), list(lengths), successful_paths\n"
      ],
      "metadata": {
        "id": "VSj1VZpWC4E4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4HvCHpqB7SyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read time before diagnosis from speakers_info.csv\n",
        "def get_time_before_diagnosis(speaker_path):\n",
        "    info_path = os.path.join(speaker_path, \"speakers_info.csv\")\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(info_path)\n",
        "        # Filter rows based on conditions\n",
        "        filtered = df[\n",
        "            (df['status'] == 'target') &\n",
        "            (df['before_after_diagnosis'] == 'before') &\n",
        "            (df['years_from_diagnosis'].between(0, 11, inclusive='neither'))\n",
        "        ]\n",
        "        if not filtered.empty:\n",
        "            return filtered['years_from_diagnosis'].iloc[0]\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {info_path}: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "SQFsrA9p7ShA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract traditional features"
      ],
      "metadata": {
        "id": "_2jtUkdCGn00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_traditional_features(audio_paths):\n",
        "    features = []\n",
        "    labels = []\n",
        "    ids = []\n",
        "    times = []\n",
        "    lengths = []\n",
        "    successful_paths = []\n",
        "\n",
        "    for audio_path in tqdm(audio_paths, desc=\"Extracting traditional features\"):\n",
        "        try:\n",
        "            # Load audio ONCE with librosa\n",
        "            audio_array, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "            patient_id = audio_path.split(\"/\")[2]\n",
        "            speaker_dir = Path(audio_path).parent\n",
        "            audio_dir = Path(speaker_dir).parent\n",
        "            time_before_diagnosis = get_time_before_diagnosis(audio_dir)\n",
        "            duration = librosa.get_duration(y=audio_array, sr=sr)\n",
        "            group = Path(audio_path).parent.parent.name\n",
        "            label = 1 if group == \"PD\" else 0\n",
        "\n",
        "            # Create parselmouth Sound object from librosa's data\n",
        "            try:\n",
        "                # Ensure 'y' is float64 for parselmouth, librosa might return float32\n",
        "                sound = parselmouth.Sound(audio_array.astype(np.float64), sampling_frequency=sr)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to create parselmouth.Sound from array for {audio_path}: {e}\")\n",
        "                # Skip this file if parselmouth object creation fails\n",
        "                continue\n",
        "\n",
        "            feature_vector = []\n",
        "\n",
        "            # Jitter features\n",
        "            try:\n",
        "                pointProcess = call(sound, \"To PointProcess (periodic, cc)\", 75, 600)\n",
        "                jitter_local = call(pointProcess, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
        "                jitter_local_absolute = call(pointProcess, \"Get jitter (local, absolute)\", 0, 0, 0.0001, 0.02, 1.3)\n",
        "                jitter_rap = call(pointProcess, \"Get jitter (rap)\", 0, 0, 0.0001, 0.02, 1.3)\n",
        "                jitter_ppq5 = call(pointProcess, \"Get jitter (ppq5)\", 0, 0, 0.0001, 0.02, 1.3)\n",
        "                feature_vector.extend([jitter_local, jitter_local_absolute, jitter_rap, jitter_ppq5])\n",
        "            except Exception as e:\n",
        "                feature_vector.extend([0, 0, 0, 0])\n",
        "\n",
        "            # Shimmer features\n",
        "            try:\n",
        "                shimmer_local = call([sound, pointProcess], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
        "                shimmer_local_db = call([sound, pointProcess], \"Get shimmer (local_dB)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
        "                feature_vector.extend([shimmer_local, shimmer_local_db])\n",
        "            except Exception as e:\n",
        "                feature_vector.extend([0, 0])\n",
        "\n",
        "            # Harmonics-to-noise ratio\n",
        "            try:\n",
        "                harmonicity = call(sound, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
        "                hnr = call(harmonicity, \"Get mean\", 0, 0)\n",
        "                feature_vector.append(hnr)\n",
        "            except Exception as e:\n",
        "                feature_vector.append(0)\n",
        "\n",
        "            # Percentage of vocalic intervals (using RMS energy)\n",
        "            rms_frames = librosa.feature.rms(y=audio_array).mean()\n",
        "            feature_vector.append(rms_frames)\n",
        "\n",
        "            # MFCCs\n",
        "            try:\n",
        "                mfccs = librosa.feature.mfcc(y=audio_array, sr=sr, n_mfcc=13)\n",
        "                mfcc_means = np.mean(mfccs, axis=1)\n",
        "                feature_vector.extend(mfcc_means)\n",
        "            except Exception as e:\n",
        "                feature_vector.extend([0] * 13)\n",
        "\n",
        "            # Spectral features\n",
        "            try:\n",
        "                spectral_centroid = librosa.feature.spectral_centroid(y=audio_array, sr=sr).mean()\n",
        "                spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio_array, sr=sr).mean()\n",
        "                feature_vector.extend([spectral_centroid, spectral_bandwidth])\n",
        "            except Exception as e:\n",
        "                feature_vector.extend([0, 0])\n",
        "\n",
        "            # F0 Statistics (Prosodic Features)\n",
        "            f0_values = []\n",
        "            try:\n",
        "                pitch = sound.to_pitch()\n",
        "                f0_values = pitch.selected_array['frequency']\n",
        "                f0_values = f0_values[f0_values != 0] # Exclude unvoiced frames (0 Hz)\n",
        "                if len(f0_values) > 0:\n",
        "                    f0_mean = np.mean(f0_values)\n",
        "                    f0_std = np.std(f0_values)\n",
        "                    f0_range = np.max(f0_values) - np.min(f0_values)\n",
        "                else:\n",
        "                    f0_mean, f0_std, f0_range = 0, 0, 0 # Handle case with no voiced frames\n",
        "\n",
        "                feature_vector.extend([f0_mean, f0_std, f0_range])\n",
        "            except Exception as e:\n",
        "                feature_vector.extend([0, 0, 0])\n",
        "\n",
        "            # Nonlinear Dynamics (RPDE, D2, DFA)\n",
        "            rpde_val, d2_val, dfa_val = 0, 0, 0 # Default to 0 if not computed\n",
        "\n",
        "            if len(f0_values) > 10: # Need enough points for nolds functions\n",
        "                try:\n",
        "                    # These can be computationally expensive, consider skipping for large files\n",
        "                    if len(f0_values) < 5000:  # Limit to prevent memory issues\n",
        "                        rpde_val = nolds.rpde(f0_values)\n",
        "                        d2_val = nolds.d2(f0_values)\n",
        "                        dfa_val = nolds.dfa(f0_values)\n",
        "                except Exception as e:\n",
        "                    pass # Keep default 0 values\n",
        "\n",
        "            feature_vector.extend([rpde_val, d2_val, dfa_val])\n",
        "\n",
        "            # Check if feature vector has consistent length\n",
        "            expected_length = 29 # Update if adding/removing features\n",
        "            if len(feature_vector) == expected_length:\n",
        "                features.append(feature_vector)\n",
        "                # Append the newly calculated values\n",
        "                labels.append(label)\n",
        "                ids.append(patient_id)\n",
        "                times.append(time_before_diagnosis)\n",
        "                lengths.append(duration)  # Using duration instead of audio_length\n",
        "                successful_paths.append(audio_path)\n",
        "            else:\n",
        "                print(f\"Skipping {audio_path} due to inconsistent feature vector length ({len(feature_vector)} != {expected_length})\")\n",
        "\n",
        "            # Force garbage collection to free memory\n",
        "            gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred processing file: {audio_path}. Error: {e}\")\n",
        "\n",
        "    features_array = np.array(features, dtype=np.float32)\n",
        "    return features_array, labels, ids, times, lengths, successful_paths  # Return all data, not just features"
      ],
      "metadata": {
        "id": "wuxIJIbsGm8t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading CSV helper functions"
      ],
      "metadata": {
        "id": "cllmJlq4ITS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New function to load features from CSV\n",
        "def load_features_from_csv(filename):\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"File not found: {filename}\")\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "    print(f\"Loading features from: {filename}\")\n",
        "    df = pd.read_csv(filename)\n",
        "    required_columns = {'label', 'id', 'time_before_diagnosis', 'length'}\n",
        "    if not required_columns.issubset(df.columns):\n",
        "        print(f\"Error: One or more required columns {required_columns} missing in {filename}\")\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "    labels = df['label'].values.astype(int)\n",
        "    speaker_ids = df['id'].values\n",
        "    times = df['time_before_diagnosis'].values\n",
        "    lengths = df['length'].values\n",
        "\n",
        "    features = df.drop(columns=['label', 'id', 'time_before_diagnosis', 'length']).values.astype(np.float32)\n",
        "    features = np.nan_to_num(features)\n",
        "\n",
        "    return features, labels, speaker_ids, times, lengths, df\n",
        "\n",
        "\n",
        "\n",
        "# Save features function\n",
        "def save_features(features, labels, ids, time_before_diagnosis, audio_length, filename):\n",
        "    \"\"\"Save features and labels to a CSV file.\"\"\"\n",
        "    if features.size == 0 or not labels:\n",
        "        print(f\"Warning: No features or labels to save for {filename}\")\n",
        "        return\n",
        "    df = pd.DataFrame(features)\n",
        "    df['label'] = labels\n",
        "    df['id'] = ids\n",
        "    df['time_before_diagnosis'] = time_before_diagnosis\n",
        "    df['length'] = audio_length\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Saved features to {filename}\")\n"
      ],
      "metadata": {
        "id": "m-2OzAVgITsK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks"
      ],
      "metadata": {
        "id": "Lxwzcv0hIXU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Deep Neural Network Model\n",
        "class DeepNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(DeepNN, self).__init__()\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_dim, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.3),\n",
        "            torch.nn.Linear(256, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.3),\n",
        "            torch.nn.Linear(128, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, 1),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class ImprovedDeepNN(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes=1, dropout=0.3):\n",
        "        super(ImprovedDeepNN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.bn4 = nn.BatchNorm1d(64)\n",
        "\n",
        "        self.output = nn.Linear(64, num_classes)  # 1 for binary, >1 for multi-class (depends on number of bins)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = F.relu(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = F.relu(self.bn4(self.fc4(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        if self.num_classes == 1:\n",
        "            return torch.sigmoid(self.output(x))  # Binary classification\n",
        "        else:\n",
        "            return self.output(x)  # Use CrossEntropyLoss with raw logits\n",
        "\n",
        "\n",
        "class EnhancedSpeechNN(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes=1, dropout=0.4, use_attention=True):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.use_attention = use_attention\n",
        "\n",
        "        # Initial feature compression\n",
        "        self.embedding = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout*0.5)\n",
        "        )\n",
        "\n",
        "        # Residual blocks with projection layers\n",
        "        self.block1 = self._make_residual_block(512, 512, dropout)\n",
        "        self.block2 = self._make_residual_block(512, 256, dropout)\n",
        "        self.block3 = self._make_residual_block(256, 128, dropout)\n",
        "\n",
        "        # Predefine projection layers for residual connections\n",
        "        self.proj1 = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256)\n",
        "        )\n",
        "        self.proj2 = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128)\n",
        "        )\n",
        "\n",
        "        if self.use_attention:\n",
        "            self.attention = nn.MultiheadAttention(128, num_heads=4, dropout=dropout)\n",
        "\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _make_residual_block(self, in_dim, out_dim, dropout):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(in_dim, out_dim),\n",
        "            nn.BatchNorm1d(out_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(out_dim, out_dim),\n",
        "            nn.BatchNorm1d(out_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def _residual_connection(self, x, identity, proj_layer=None):\n",
        "        \"\"\"Handle residual connection with optional projection\"\"\"\n",
        "        if proj_layer is not None:\n",
        "            identity = proj_layer(identity)\n",
        "        return x + identity\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu', a=0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial embedding\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Block 1 (512 -> 512)\n",
        "        identity = x\n",
        "        x = self.block1(x)\n",
        "        x = self._residual_connection(x, identity)\n",
        "\n",
        "        # Block 2 (512 -> 256)\n",
        "        identity = x\n",
        "        x = self.block2(x)\n",
        "        x = self._residual_connection(x, identity, self.proj1)\n",
        "\n",
        "        # Block 3 (256 -> 128)\n",
        "        identity = x\n",
        "        x = self.block3(x)\n",
        "        x = self._residual_connection(x, identity, self.proj2)\n",
        "\n",
        "        # Attention\n",
        "        if self.use_attention:\n",
        "            x_attn, _ = self.attention(x.unsqueeze(1), x.unsqueeze(1), x.unsqueeze(1))\n",
        "            x = x + x_attn.squeeze(1)\n",
        "\n",
        "        # Final processing\n",
        "        x = self.final(x)\n",
        "\n",
        "        return torch.sigmoid(x) if self.num_classes == 1 else x\n",
        "\n",
        "\n",
        "class SpeechPDDetector(torch.nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SpeechPDDetector, self).__init__()\n",
        "\n",
        "        # Initial feature extraction\n",
        "        self.feature_extraction = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_dim, 512),\n",
        "            torch.nn.BatchNorm1d(512),\n",
        "            torch.nn.LeakyReLU(0.2),\n",
        "            torch.nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Deeper representation layers with residual connections\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512, 384),\n",
        "            torch.nn.BatchNorm1d(384),\n",
        "            torch.nn.LeakyReLU(0.2),\n",
        "            torch.nn.Dropout(0.4)\n",
        "        )\n",
        "\n",
        "        self.layer2 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(384, 256),\n",
        "            torch.nn.BatchNorm1d(256),\n",
        "            torch.nn.LeakyReLU(0.2),\n",
        "            torch.nn.Dropout(0.4)\n",
        "        )\n",
        "\n",
        "        self.layer3 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(256, 128),\n",
        "            torch.nn.BatchNorm1d(128),\n",
        "            torch.nn.LeakyReLU(0.2),\n",
        "            torch.nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = torch.nn.Sequential(\n",
        "            torch.nn.Linear(128, 64),\n",
        "            torch.nn.BatchNorm1d(64),\n",
        "            torch.nn.LeakyReLU(0.2),\n",
        "            torch.nn.Dropout(0.3),\n",
        "            torch.nn.Linear(64, 32),\n",
        "            torch.nn.BatchNorm1d(32),\n",
        "            torch.nn.LeakyReLU(0.2),\n",
        "            torch.nn.Linear(32, 1),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Residual connections\n",
        "        self.res_connection1 = torch.nn.Linear(input_dim, 512)\n",
        "        self.res_connection2 = torch.nn.Linear(512, 256)\n",
        "        self.res_connection3 = torch.nn.Linear(256, 128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial feature extraction with residual connection\n",
        "        res1 = self.res_connection1(x)\n",
        "        features = self.feature_extraction(x) + res1\n",
        "\n",
        "        # Layer 1\n",
        "        layer1_out = self.layer1(features)\n",
        "\n",
        "        # Layer 2 with residual connection\n",
        "        res2 = self.res_connection2(features)\n",
        "        layer2_out = self.layer2(layer1_out) + res2\n",
        "\n",
        "        # Layer 3 with residual connection\n",
        "        res3 = self.res_connection3(layer2_out)\n",
        "        layer3_out = self.layer3(layer2_out) + res3\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(layer3_out)\n",
        "\n",
        "        return output\n",
        "\n",
        "class SpeechPDDetectorWithAttention(torch.nn.Module):\n",
        "    def __init__(self, input_dim, use_attention=True):\n",
        "        super(SpeechPDDetectorWithAttention, self).__init__()  # This line was wrong\n",
        "\n",
        "        # Initial feature extraction\n",
        "        self.feature_extraction = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_dim, 512),\n",
        "            torch.nn.BatchNorm1d(512),\n",
        "            torch.nn.LeakyReLU(0.2),\n",
        "            torch.nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Deeper representation layers\n",
        "        self.deep_layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512, 384),\n",
        "            torch.nn.BatchNorm1d(384),\n",
        "            torch.nn.LeakyReLU(0.2),\n",
        "            torch.nn.Dropout(0.4),\n",
        "\n",
        "            torch.nn.Linear(384, 256),\n",
        "            torch.nn.BatchNorm1d(256),\n",
        "            torch.nn.LeakyReLU(0.2),\n",
        "            torch.nn.Dropout(0.4),\n",
        "\n",
        "            torch.nn.Linear(256, 128),\n",
        "            torch.nn.BatchNorm1d(128),\n",
        "            torch.nn.LeakyReLU(0.2),\n",
        "            torch.nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Optional attention mechanism\n",
        "        self.use_attention = use_attention\n",
        "        if use_attention:\n",
        "            self.attention = torch.nn.Sequential(\n",
        "                torch.nn.Linear(128, 64),\n",
        "                torch.nn.Tanh(),\n",
        "                torch.nn.Linear(64, 1)\n",
        "            )\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = torch.nn.Sequential(\n",
        "            torch.nn.Linear(128, 64),\n",
        "            torch.nn.BatchNorm1d(64),\n",
        "            torch.nn.LeakyReLU(0.2),\n",
        "            torch.nn.Linear(64, 32),\n",
        "            torch.nn.BatchNorm1d(32),\n",
        "            torch.nn.LeakyReLU(0.2),\n",
        "            torch.nn.Linear(32, 1),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Residual connections\n",
        "        self.res_connection1 = torch.nn.Linear(input_dim, 512)\n",
        "        self.res_connection2 = torch.nn.Linear(512, 128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feature extraction with residual connection\n",
        "        res1 = self.res_connection1(x)\n",
        "        features = self.feature_extraction(x) + res1\n",
        "\n",
        "        # Deep representation with residual connection\n",
        "        res2 = self.res_connection2(features)\n",
        "        deep_features = self.deep_layers(features) + res2\n",
        "\n",
        "        # Apply attention if enabled\n",
        "        if self.use_attention:\n",
        "            attention_weights = torch.softmax(self.attention(deep_features), dim=1)\n",
        "            attended_features = deep_features * attention_weights\n",
        "            output = self.classifier(attended_features)\n",
        "        else:\n",
        "            output = self.classifier(deep_features)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "DznagIoBIYoQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training/Testing/Comparison Helper Functions"
      ],
      "metadata": {
        "id": "83TD4k9YaXrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(y_true, y_pred_prob):\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"roc_auc\": roc_auc_score(y_true, y_pred_prob) if len(np.unique(y_true)) > 1 else np.nan,\n",
        "        \"sensitivity\": recall_score(y_true, y_pred),\n",
        "        \"specificity\": recall_score(1 - y_true, 1 - y_pred),\n",
        "        \"f1_score\": f1_score(y_true, y_pred),\n",
        "        \"confusion_matrix\": confusion_matrix(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "def average_metrics(results_list):\n",
        "    return {\n",
        "        key: np.nanmean([res[key] for res in results_list])\n",
        "        for key in results_list[0].keys()\n",
        "    }\n",
        "\n",
        "def empty_metrics():\n",
        "    return {\n",
        "        \"accuracy\": np.nan,\n",
        "        \"roc_auc\": np.nan,\n",
        "        \"sensitivity\": np.nan,\n",
        "        \"specificity\": np.nan,\n",
        "        \"f1_score\": np.nan,\n",
        "        \"confusion_matrix\": np.array([[0, 0], [0, 0]])\n",
        "    }"
      ],
      "metadata": {
        "id": "ATb3Un62aX7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and testing DNN"
      ],
      "metadata": {
        "id": "6pZtXFUrIh9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evaluate_model(X_train, y_train, X_test, y_test, feature_type, model_to_use):\n",
        "    \"\"\"Train and evaluate model for specific feature type\"\"\"\n",
        "    if X_train.size == 0 or X_test.size == 0:\n",
        "        print(f\"Warning: Empty feature sets for {feature_type}. Skipping evaluation.\")\n",
        "        return empty_metrics()\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Create model with appropriate input dimensions\n",
        "    input_dim = X_train.shape[1]\n",
        "    model = model_to_use(input_dim).to(device)\n",
        "\n",
        "    # Rest of training logic remains the same\n",
        "    criterion = torch.nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Convert data to tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1).to(device)\n",
        "\n",
        "    # Training loop\n",
        "    dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n",
        "\n",
        "    for epoch in range(100):\n",
        "        model.train()\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred_prob = model(X_test_tensor).cpu().numpy()\n",
        "\n",
        "    return calculate_metrics(y_test, y_pred_prob)"
      ],
      "metadata": {
        "id": "5Kmd_vs_IiQL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model + Feature comparison"
      ],
      "metadata": {
        "id": "bhMR2mh_Ixx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crossval_compare_features(dataset_path, k=5, save_figures=True, min_audio_length=0, model=None):\n",
        "    save_folder = f'visualization_results_{model}'\n",
        "    # Create directory for saving figures if needed\n",
        "    os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "    # Load data with proper alignment\n",
        "    all_audio_paths, all_labels, all_speaker_ids = load_parkceleb_dataset_with_speakers(dataset_path)\n",
        "\n",
        "    # Load features\n",
        "    whisper_data = load_features_from_csv(\"whisper_all_features.csv\")\n",
        "    traditional_data = load_features_from_csv(\"traditional_all_features.csv\")\n",
        "\n",
        "    # Dictionary to store all results\n",
        "    results = {}\n",
        "    all_fold_metrics = {}  # Store metrics for each fold for later visualization\n",
        "\n",
        "    for feature_type, (features, labels, speakers, times, lengths, _) in [\n",
        "        (\"Whisper\", whisper_data),\n",
        "        (\"Traditional\", traditional_data)\n",
        "    ]:\n",
        "        if features is None:\n",
        "            continue\n",
        "\n",
        "        length_mask = lengths >= min_audio_length\n",
        "        features = features[length_mask]\n",
        "        labels = labels[length_mask]\n",
        "        speakers = speakers[length_mask]\n",
        "        times = times[length_mask]\n",
        "        lengths = lengths[length_mask]\n",
        "\n",
        "        print(f\"\\n=== Processing {feature_type} Features ===\")\n",
        "        print(f\"Feature dimension: {features.shape[1]}\")\n",
        "        print(f\"Class distribution: {np.bincount(labels)}\")\n",
        "\n",
        "        # Speaker-level stratified K-Fold\n",
        "        unique_speakers = np.unique(speakers)\n",
        "        speaker_labels = np.array([1 if 'pd' in s else 0 for s in unique_speakers])\n",
        "\n",
        "        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "        fold_results = []\n",
        "        fold_metrics_dict = {\n",
        "            'accuracy': [], 'roc_auc': [], 'sensitivity': [],\n",
        "            'specificity': [], 'f1_score': [], 'confusion_matrices': [],\n",
        "            'y_test': [], 'y_pred_prob': []  # Store for ROC curve generation\n",
        "        }\n",
        "\n",
        "        for fold, (train_idx, test_idx) in enumerate(skf.split(unique_speakers, speaker_labels)):\n",
        "            print(f\"\\n=== Fold {fold+1} ===\")\n",
        "            train_speakers = set(unique_speakers[train_idx])\n",
        "            test_speakers = set(unique_speakers[test_idx])\n",
        "\n",
        "            # Select data for current fold\n",
        "            train_mask = np.isin(speakers, list(train_speakers))\n",
        "            test_mask = np.isin(speakers, list(test_speakers))\n",
        "\n",
        "            X_train, X_test = features[train_mask], features[test_mask]\n",
        "            y_train, y_test = labels[train_mask], labels[test_mask]\n",
        "\n",
        "            # Check for valid class distribution\n",
        "            if len(np.unique(y_test)) < 2:\n",
        "                print(f\"Skipping fold {fold+1} - single class in test set\")\n",
        "                continue\n",
        "\n",
        "            # Standardize features\n",
        "            scaler = StandardScaler()\n",
        "            X_train = scaler.fit_transform(X_train)\n",
        "            X_test = scaler.transform(X_test)\n",
        "\n",
        "            # Train and evaluate using your existing function\n",
        "            metrics = train_evaluate_model(X_train, y_train, X_test, y_test, feature_type, model)\n",
        "            fold_results.append(metrics)\n",
        "\n",
        "            # Extract y_pred_prob for ROC curve generation (we'll need to regenerate it)\n",
        "            y_pred = (metrics['confusion_matrix'][:, 1].sum(), metrics['confusion_matrix'][:, 0].sum())\n",
        "\n",
        "            # Store detailed metrics for visualization\n",
        "            for key in ['accuracy', 'roc_auc', 'sensitivity', 'specificity', 'f1_score']:\n",
        "                fold_metrics_dict[key].append(metrics[key])\n",
        "            fold_metrics_dict['confusion_matrices'].append(metrics['confusion_matrix'])\n",
        "            fold_metrics_dict['y_test'].append(y_test)\n",
        "\n",
        "            # Visualize confusion matrix for this fold\n",
        "            plt.figure(figsize=(6, 5))\n",
        "            cm = metrics['confusion_matrix']\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                        xticklabels=['Control', 'PD'], yticklabels=['Control', 'PD'])\n",
        "            plt.xlabel('Predicted')\n",
        "            plt.ylabel('True')\n",
        "            plt.title(f'{feature_type} - Fold {fold+1} Confusion Matrix')\n",
        "            if save_figures:\n",
        "                plt.savefig(f'{save_folder}/{feature_type}_fold{fold+1}_confusion.png', bbox_inches='tight')\n",
        "                plt.close()\n",
        "            else:\n",
        "                plt.show()\n",
        "\n",
        "        # Store results\n",
        "        if fold_results:\n",
        "            results[feature_type] = average_metrics(fold_results)\n",
        "            all_fold_metrics[feature_type] = fold_metrics_dict\n",
        "\n",
        "            # Visualize metrics across folds\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            metrics_across_folds = pd.DataFrame({\n",
        "                'Accuracy': fold_metrics_dict['accuracy'],\n",
        "                'AUC': fold_metrics_dict['roc_auc'],\n",
        "                'Sensitivity': fold_metrics_dict['sensitivity'],\n",
        "                'Specificity': fold_metrics_dict['specificity'],\n",
        "                'F1 Score': fold_metrics_dict['f1_score']\n",
        "            })\n",
        "\n",
        "            metrics_across_folds.index = [f'Fold {i+1}' for i in range(len(metrics_across_folds))]\n",
        "            ax = metrics_across_folds.plot(kind='bar', figsize=(10, 6))\n",
        "            plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.3)  # baseline\n",
        "            plt.title(f'{feature_type} - Performance Metrics Across Folds')\n",
        "            plt.ylabel('Score')\n",
        "            plt.ylim(0, 1.05)\n",
        "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "            plt.legend(loc='lower center', bbox_to_anchor=(0.5, -0.25), ncol=5)\n",
        "\n",
        "            # Add value labels on bars\n",
        "            for container in ax.containers:\n",
        "                ax.bar_label(container, fmt='%.2f', fontsize=8)\n",
        "\n",
        "            if save_figures:\n",
        "                plt.savefig(f'{save_folder}/{feature_type}_metrics_by_fold.png', bbox_inches='tight')\n",
        "                plt.close()\n",
        "            else:\n",
        "                plt.show()\n",
        "\n",
        "            # Visualize average confusion matrix\n",
        "            plt.figure(figsize=(6, 5))\n",
        "            avg_cm = np.mean([cm for cm in fold_metrics_dict['confusion_matrices']], axis=0)\n",
        "            sns.heatmap(avg_cm, annot=True, fmt='.1f', cmap='Blues',\n",
        "                        xticklabels=['Control', 'PD'], yticklabels=['Control', 'PD'])\n",
        "            plt.xlabel('Predicted')\n",
        "            plt.ylabel('True')\n",
        "            plt.title(f'{feature_type} - Average Confusion Matrix')\n",
        "            if save_figures:\n",
        "                plt.savefig(f'{save_folder}/{feature_type}_avg_confusion.png', bbox_inches='tight')\n",
        "                plt.close()\n",
        "            else:\n",
        "                plt.show()\n",
        "\n",
        "\n",
        "        else:\n",
        "            results[feature_type] = empty_metrics()\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n=== Final Results ===\")\n",
        "    for feature_type, metrics in results.items():\n",
        "        print(f\"\\n{feature_type} Features:\")\n",
        "        for metric, value in metrics.items():\n",
        "            if metric != 'confusion_matrix':\n",
        "                print(f\"{metric:15}: {value:.4f}\")\n",
        "\n",
        "    # Compare feature types if both are available\n",
        "    if len(results) > 1:\n",
        "        # Create comparison bar chart\n",
        "        metric_names = ['Accuracy', 'AUC', 'Sensitivity', 'Specificity', 'F1 Score']\n",
        "        feature_types = list(results.keys())\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        width = 0.35\n",
        "        x = np.arange(len(metric_names))\n",
        "\n",
        "        for i, feature_type in enumerate(feature_types):\n",
        "            metrics_values = [\n",
        "                results[feature_type]['accuracy'],\n",
        "                results[feature_type]['roc_auc'],\n",
        "                results[feature_type]['sensitivity'],\n",
        "                results[feature_type]['specificity'],\n",
        "                results[feature_type]['f1_score']\n",
        "            ]\n",
        "            plt.bar(x + (i - 0.5*(len(feature_types)-1)) * width, metrics_values,\n",
        "                   width, label=feature_type)\n",
        "\n",
        "            # Add value labels on bars\n",
        "            for j, value in enumerate(metrics_values):\n",
        "                plt.text(x[j] + (i - 0.5*(len(feature_types)-1)) * width, value + 0.01,\n",
        "                         f'{value:.3f}', ha='center', fontsize=8)\n",
        "\n",
        "        plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.3)  # baseline\n",
        "        plt.xlabel('Metric')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('Performance Comparison Between Feature Types')\n",
        "        plt.xticks(x, metric_names)\n",
        "        plt.ylim(0, 1.05)\n",
        "        plt.legend()\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "        if save_figures:\n",
        "            plt.savefig(f'{save_folder}/feature_comparison.png', bbox_inches='tight')\n",
        "            plt.close()\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "BywVXbGnGpTb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = crossval_compare_features(\"parkceleb_filtered\", min_audio_length=0.1, model=SpeechPDDetector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "Y1UFsHNvHdu6",
        "outputId": "af512c9b-8ab4-45dc-e787-6f441e683416"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading features from: whisper_all_features.csv\n",
            "Loading features from: traditional_all_features.csv\n",
            "\n",
            "=== Processing Whisper Features ===\n",
            "Feature dimension: 768\n",
            "Class distribution: [5239 8614]\n",
            "\n",
            "=== Fold 1 ===\n",
            "\n",
            "=== Fold 2 ===\n",
            "\n",
            "=== Fold 3 ===\n",
            "\n",
            "=== Fold 4 ===\n",
            "\n",
            "=== Fold 5 ===\n",
            "\n",
            "=== Processing Traditional Features ===\n",
            "Feature dimension: 29\n",
            "Class distribution: [5239 8614]\n",
            "\n",
            "=== Fold 1 ===\n",
            "\n",
            "=== Fold 2 ===\n",
            "\n",
            "=== Fold 3 ===\n",
            "\n",
            "=== Fold 4 ===\n",
            "\n",
            "=== Fold 5 ===\n",
            "\n",
            "=== Final Results ===\n",
            "\n",
            "Whisper Features:\n",
            "accuracy       : 0.5500\n",
            "roc_auc        : 0.5402\n",
            "sensitivity    : 0.5666\n",
            "specificity    : 0.5006\n",
            "f1_score       : 0.5984\n",
            "\n",
            "Traditional Features:\n",
            "accuracy       : 0.5361\n",
            "roc_auc        : 0.5404\n",
            "sensitivity    : 0.5565\n",
            "specificity    : 0.5026\n",
            "f1_score       : 0.5901\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = crossval_compare_features(\"parkceleb_filtered\", min_audio_length=0.1, model=ImprovedDeepNN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "f23TWKETHfkp",
        "outputId": "4b50babd-60fd-47e8-d71a-6066ce592834"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading features from: whisper_all_features.csv\n",
            "Loading features from: traditional_all_features.csv\n",
            "\n",
            "=== Processing Whisper Features ===\n",
            "Feature dimension: 768\n",
            "Class distribution: [5239 8614]\n",
            "\n",
            "=== Fold 1 ===\n",
            "\n",
            "=== Fold 2 ===\n",
            "\n",
            "=== Fold 3 ===\n",
            "\n",
            "=== Fold 4 ===\n",
            "\n",
            "=== Fold 5 ===\n",
            "\n",
            "=== Processing Traditional Features ===\n",
            "Feature dimension: 29\n",
            "Class distribution: [5239 8614]\n",
            "\n",
            "=== Fold 1 ===\n",
            "\n",
            "=== Fold 2 ===\n",
            "\n",
            "=== Fold 3 ===\n",
            "\n",
            "=== Fold 4 ===\n",
            "\n",
            "=== Fold 5 ===\n",
            "\n",
            "=== Final Results ===\n",
            "\n",
            "Whisper Features:\n",
            "accuracy       : 0.5615\n",
            "roc_auc        : 0.5580\n",
            "sensitivity    : 0.5544\n",
            "specificity    : 0.5388\n",
            "f1_score       : 0.5986\n",
            "\n",
            "Traditional Features:\n",
            "accuracy       : 0.5486\n",
            "roc_auc        : 0.5803\n",
            "sensitivity    : 0.5476\n",
            "specificity    : 0.5598\n",
            "f1_score       : 0.5931\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = crossval_compare_features(\"parkceleb_filtered\", min_audio_length=0.1, model=DeepNN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "WU1MTjz2P0rL",
        "outputId": "6c9d6b8d-a9e6-4044-f5e0-3d545fd130a4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading features from: whisper_all_features.csv\n",
            "Loading features from: traditional_all_features.csv\n",
            "\n",
            "=== Processing Whisper Features ===\n",
            "Feature dimension: 768\n",
            "Class distribution: [5239 8614]\n",
            "\n",
            "=== Fold 1 ===\n",
            "\n",
            "=== Fold 2 ===\n",
            "\n",
            "=== Fold 3 ===\n",
            "\n",
            "=== Fold 4 ===\n",
            "\n",
            "=== Fold 5 ===\n",
            "\n",
            "=== Processing Traditional Features ===\n",
            "Feature dimension: 29\n",
            "Class distribution: [5239 8614]\n",
            "\n",
            "=== Fold 1 ===\n",
            "\n",
            "=== Fold 2 ===\n",
            "\n",
            "=== Fold 3 ===\n",
            "\n",
            "=== Fold 4 ===\n",
            "\n",
            "=== Fold 5 ===\n",
            "\n",
            "=== Final Results ===\n",
            "\n",
            "Whisper Features:\n",
            "accuracy       : 0.5451\n",
            "roc_auc        : 0.5423\n",
            "sensitivity    : 0.5396\n",
            "specificity    : 0.5281\n",
            "f1_score       : 0.5838\n",
            "\n",
            "Traditional Features:\n",
            "accuracy       : 0.5557\n",
            "roc_auc        : 0.5820\n",
            "sensitivity    : 0.5508\n",
            "specificity    : 0.5613\n",
            "f1_score       : 0.5991\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = crossval_compare_features(\"parkceleb_filtered\", min_audio_length=0.1, model=SpeechPDDetectorWithAttention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "HHAXGf50SRm-",
        "outputId": "179b2ab9-2d7f-4a5b-b0c7-464a754ffec6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading features from: whisper_all_features.csv\n",
            "Loading features from: traditional_all_features.csv\n",
            "\n",
            "=== Processing Whisper Features ===\n",
            "Feature dimension: 768\n",
            "Class distribution: [5239 8614]\n",
            "\n",
            "=== Fold 1 ===\n",
            "\n",
            "=== Fold 2 ===\n",
            "\n",
            "=== Fold 3 ===\n",
            "\n",
            "=== Fold 4 ===\n",
            "\n",
            "=== Fold 5 ===\n",
            "\n",
            "=== Processing Traditional Features ===\n",
            "Feature dimension: 29\n",
            "Class distribution: [5239 8614]\n",
            "\n",
            "=== Fold 1 ===\n",
            "\n",
            "=== Fold 2 ===\n",
            "\n",
            "=== Fold 3 ===\n",
            "\n",
            "=== Fold 4 ===\n",
            "\n",
            "=== Fold 5 ===\n",
            "\n",
            "=== Final Results ===\n",
            "\n",
            "Whisper Features:\n",
            "accuracy       : 0.5214\n",
            "roc_auc        : 0.5209\n",
            "sensitivity    : 0.5143\n",
            "specificity    : 0.5189\n",
            "f1_score       : 0.5611\n",
            "\n",
            "Traditional Features:\n",
            "accuracy       : 0.5406\n",
            "roc_auc        : 0.5638\n",
            "sensitivity    : 0.5854\n",
            "specificity    : 0.4902\n",
            "f1_score       : 0.6022\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sanity checks\n",
        "I messed up the save to csv part of whisper feature generation, so I had to fix it. That's what the code below is for. The function is also fixed now."
      ],
      "metadata": {
        "id": "rtenXgkUM34r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"whisper_all_features.csv\")\n",
        "print(\"Unique speaker IDs:\", df['id'].unique())"
      ],
      "metadata": {
        "id": "cV2aBcE7M53w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.copy2(\"whisper_all_features.csv\", \"whisper_all_features_BACKUP.csv\")\n",
        "shutil.copy2(\"traditional_all_features.csv\", \"traditional_all_features_BACKUP.csv\")"
      ],
      "metadata": {
        "id": "tmc4pCkrM8W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"whisper_all_features.csv\")\n",
        "print(\"PD count:\", df[df['label'] == 1].shape[0])\n",
        "print(\"CN count:\", df[df['label'] == 0].shape[0])\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"traditional_all_features.csv\")\n",
        "print(\"PD count:\", df[df['label'] == 1].shape[0])\n",
        "print(\"CN count:\", df[df['label'] == 0].shape[0])"
      ],
      "metadata": {
        "id": "ieh1zB8gNE8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_labels_in_csv(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Create labels based on 'id' column\n",
        "    df['label'] = df['id'].str.contains('pd', case=False).astype(int)\n",
        "\n",
        "    # Save fixed CSV (backup original first!)\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"Updated labels in {csv_path}. Class distribution: {df['label'].value_counts().to_dict()}\")\n",
        "\n",
        "# Apply to both feature CSVs\n",
        "fix_labels_in_csv(\"whisper_all_features.csv\")\n",
        "# fix_labels_in_csv(\"traditional_all_features.csv\")"
      ],
      "metadata": {
        "id": "lpo2UKDDM3tk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}