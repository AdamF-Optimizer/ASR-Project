{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXdHuMT7K5or"
      },
      "outputs": [],
      "source": [
        "# ! pip install librosa\n",
        "# ! pip install parselmouth\n",
        "# ! pip install pyAudioAnalysis\n",
        "# ! pip install torch\n",
        "# ! pip install transformers\n",
        "# ! pip install pandas\n",
        "# ! pip install tqdm\n",
        "# ! pip install scikit-learn\n",
        "# ! pip install parselmouth\n",
        "# ! pip install nolds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fni6FlZtKC5P",
        "outputId": "89f91768-6270-4fcc-f4fd-207a0f362c68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Project\n"
          ]
        }
      ],
      "source": [
        "%cd Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aa0bO-l17EdV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "import librosa\n",
        "import parselmouth\n",
        "from parselmouth.praat import call\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score, precision_score, roc_auc_score, roc_curve, auc\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoFeatureExtractor, WhisperModel, AutoModelForAudioClassification\n",
        "import logging\n",
        "import gc\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nolds\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-VfFLIIKwYQ",
        "outputId": "15d67c43-578a-480c-d768-bb4e8891bf3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8708 Parkinson files and 5349 Control files.\n"
          ]
        }
      ],
      "source": [
        "base_path = \"ParkCeleb_filtered\"\n",
        "groups = {\"PD\": 1, \"CN\": 0}\n",
        "\n",
        "parkinson_files = []\n",
        "control_files = []\n",
        "\n",
        "# Loop over each group (PD, CN)\n",
        "for group, label in groups.items():\n",
        "    group_path = os.path.join(base_path, group)\n",
        "    # Recursively search for any .wav file\n",
        "    wav_files = glob(os.path.join(group_path, \"**\", \"*.wav\"), recursive=True)\n",
        "\n",
        "    if label == 1:\n",
        "        parkinson_files.extend(wav_files)\n",
        "    else:\n",
        "        control_files.extend(wav_files)\n",
        "\n",
        "# Merge and create labels\n",
        "all_files = parkinson_files + control_files\n",
        "labels = [1] * len(parkinson_files) + [0] * len(control_files)\n",
        "\n",
        "print(f\"Found {len(parkinson_files)} Parkinson files and {len(control_files)} Control files.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VboBLoTI-Rlj"
      },
      "source": [
        "# Load ParkCeleb dataset and extract speaker IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_I5raZBx-R2X"
      },
      "outputs": [],
      "source": [
        "def load_parkceleb_dataset_with_speakers(dataset_path):\n",
        "    groups = {\"PD\": 1, \"CN\": 0}\n",
        "    audio_files = []\n",
        "    labels = []\n",
        "    speaker_ids = []\n",
        "\n",
        "    # Loop over each group (PD, CN)\n",
        "    for group, label in groups.items():\n",
        "        group_path = os.path.join(dataset_path, group)\n",
        "        # Recursively search for the specific file\n",
        "        for audio_file in glob(os.path.join(group_path, \"**\", \"*.wav\"), recursive=True):\n",
        "            audio_files.append(audio_file)\n",
        "            labels.append(label)\n",
        "\n",
        "            # Extract speaker ID from the path (assuming structure where speaker ID is the directory name after the group name e.g. CN/cn_01 or PD/pd_01)\n",
        "            try:\n",
        "                # Example: dataset_path/PD/speaker01/utterance_1.wav\n",
        "                relative_path = os.path.relpath(audio_file, group_path)\n",
        "                speaker_id = relative_path.split(os.sep)[0]\n",
        "                speaker_ids.append(speaker_id)\n",
        "            except IndexError:\n",
        "                print(f\"Could not extract speaker ID from path: {audio_file}\")\n",
        "                speaker_ids.append(None) # Handle cases where speaker ID cannot be extracted, caused by errors in dataset\n",
        "\n",
        "    # Filter out files where speaker ID couldn't be determined, some stupid bug\n",
        "    valid_indices = [i for i, spkr_id in enumerate(speaker_ids) if spkr_id is not None]\n",
        "    audio_files = [audio_files[i] for i in valid_indices]\n",
        "    labels = [labels[i] for i in valid_indices]\n",
        "    speaker_ids = [speaker_ids[i] for i in valid_indices]\n",
        "\n",
        "    return audio_files, labels, speaker_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFbQrRZpC35N"
      },
      "source": [
        "# Create whisper features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VSj1VZpWC4E4"
      },
      "outputs": [],
      "source": [
        "# Read time before diagnosis from speakers_info.csv\n",
        "def get_time_before_diagnosis(speaker_path):\n",
        "    info_path = os.path.join(speaker_path, \"speakers_info.csv\")\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(info_path)\n",
        "        # Filter rows based on conditions\n",
        "        filtered = df[\n",
        "            (df['status'] == 'target') &\n",
        "            (df['before_after_diagnosis'] == 'before') &\n",
        "            (df['years_from_diagnosis'].between(0, 11, inclusive='neither'))\n",
        "        ]\n",
        "        if not filtered.empty:\n",
        "            return filtered['years_from_diagnosis'].iloc[0]\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {info_path}: {e}\")\n",
        "    return None\n",
        "\n",
        "def extract_whisper_features(audio_paths, model_name=\"openai/whisper-small\"):\n",
        "    model = WhisperModel.from_pretrained(model_name)\n",
        "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    features = []\n",
        "    labels = []\n",
        "    ids = []\n",
        "    times = []\n",
        "    lengths = []\n",
        "    successful_paths = []\n",
        "\n",
        "    for audio_path in tqdm(audio_paths, desc=\"Extracting Whisper features\"):\n",
        "        try:\n",
        "            audio_array, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
        "            # print(audio_path)\n",
        "\n",
        "            # Extract features using the feature extractor\n",
        "            inputs = feature_extractor(\n",
        "                audio_array,\n",
        "                sampling_rate=16000,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            # Define a decoder input for Whisper (needed for the forward pass structure)\n",
        "            # This is not really used, but whisper requires it to be passed, regardless if you do or do not use the decoder.\n",
        "            decoder_input_ids = torch.tensor([[1] * 100]).to(device) # Add 100 tokens for the decoder input\n",
        "\n",
        "            # Move inputs to the correct device\n",
        "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "            # Forward pass to get hidden states\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n",
        "\n",
        "            # Get the last hidden state from the encoder and average across the sequence dimension\n",
        "            embeddings = outputs.encoder_hidden_states[-1].mean(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "\n",
        "            # get patient id (pd_01 etc)\n",
        "            patient_id = audio_path.split(\"/\")[2]\n",
        "\n",
        "            # Get time before diagnosis\n",
        "            speaker_dir = Path(audio_path).parent\n",
        "            audio_dir = Path(speaker_dir).parent\n",
        "            time_before_diagnosis = get_time_before_diagnosis(audio_dir)\n",
        "\n",
        "            group = Path(audio_path).parent.parent.name  # Parent directory of speaker dir\n",
        "            label = 1 if group == \"PD\" else 0  # Directly use group name\n",
        "\n",
        "            features.append(embeddings)\n",
        "            successful_paths.append(audio_path)\n",
        "            labels.append(label)  # Use corrected label\n",
        "            ids.append(patient_id)\n",
        "            times.append(time_before_diagnosis)\n",
        "            lengths.append(librosa.get_duration(y=audio_array, sr=sr))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to process {audio_path}: {e}\")\n",
        "\n",
        "    if not features:\n",
        "        return np.array([]), [] # Return empty arrays if no features were extracted\n",
        "\n",
        "    return np.array(features), list(labels), list(ids), list(times), list(lengths), successful_paths\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2jtUkdCGn00"
      },
      "source": [
        "# Extract traditional features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wuxIJIbsGm8t"
      },
      "outputs": [],
      "source": [
        "def extract_traditional_features(audio_paths):\n",
        "    features = []\n",
        "    labels = []\n",
        "    ids = []\n",
        "    times = []\n",
        "    lengths = []\n",
        "    successful_paths = []\n",
        "\n",
        "    for audio_path in tqdm(audio_paths, desc=\"Extracting traditional features\"):\n",
        "        try:\n",
        "            # Load audio ONCE with librosa\n",
        "            audio_array, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "            patient_id = audio_path.split(\"/\")[2]\n",
        "            speaker_dir = Path(audio_path).parent\n",
        "            audio_dir = Path(speaker_dir).parent\n",
        "            time_before_diagnosis = get_time_before_diagnosis(audio_dir)\n",
        "            duration = librosa.get_duration(y=audio_array, sr=sr)\n",
        "            group = Path(audio_path).parent.parent.name\n",
        "            label = 1 if group == \"PD\" else 0\n",
        "\n",
        "            # Create parselmouth Sound object from librosa's data\n",
        "            try:\n",
        "                # Ensure 'audio_array' is float64 for parselmouth, librosa might return float32\n",
        "                sound = parselmouth.Sound(audio_array.astype(np.float64), sampling_frequency=sr)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to create parselmouth.Sound from array for {audio_path}: {e}\")\n",
        "                # Skip this file if parselmouth object creation fails\n",
        "                continue\n",
        "\n",
        "            feature_vector = []\n",
        "\n",
        "            # Jitter features\n",
        "            try:\n",
        "                pointProcess = call(sound, \"To PointProcess (periodic, cc)\", 75, 600)\n",
        "                jitter_local = call(pointProcess, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
        "                jitter_local_absolute = call(pointProcess, \"Get jitter (local, absolute)\", 0, 0, 0.0001, 0.02, 1.3)\n",
        "                jitter_rap = call(pointProcess, \"Get jitter (rap)\", 0, 0, 0.0001, 0.02, 1.3)\n",
        "                jitter_ppq5 = call(pointProcess, \"Get jitter (ppq5)\", 0, 0, 0.0001, 0.02, 1.3)\n",
        "                feature_vector.extend([jitter_local, jitter_local_absolute, jitter_rap, jitter_ppq5])\n",
        "            except Exception as e:\n",
        "                feature_vector.extend([0, 0, 0, 0])\n",
        "\n",
        "            # Shimmer features\n",
        "            try:\n",
        "                shimmer_local = call([sound, pointProcess], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
        "                shimmer_local_db = call([sound, pointProcess], \"Get shimmer (local_dB)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
        "                feature_vector.extend([shimmer_local, shimmer_local_db])\n",
        "            except Exception as e:\n",
        "                feature_vector.extend([0, 0])\n",
        "\n",
        "            # Harmonics-to-noise ratio\n",
        "            try:\n",
        "                harmonicity = call(sound, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
        "                hnr = call(harmonicity, \"Get mean\", 0, 0)\n",
        "                feature_vector.append(hnr)\n",
        "            except Exception as e:\n",
        "                feature_vector.append(0)\n",
        "\n",
        "            # Percentage of vocalic intervals (using RMS energy)\n",
        "            rms_frames = librosa.feature.rms(y=audio_array).mean()\n",
        "            feature_vector.append(rms_frames)\n",
        "\n",
        "            # MFCCs\n",
        "            try:\n",
        "                mfccs = librosa.feature.mfcc(y=audio_array, sr=sr, n_mfcc=13)\n",
        "                mfcc_means = np.mean(mfccs, axis=1)\n",
        "                feature_vector.extend(mfcc_means)\n",
        "            except Exception as e:\n",
        "                feature_vector.extend([0] * 13)\n",
        "\n",
        "            # Spectral features\n",
        "            try:\n",
        "                spectral_centroid = librosa.feature.spectral_centroid(y=audio_array, sr=sr).mean()\n",
        "                spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio_array, sr=sr).mean()\n",
        "                feature_vector.extend([spectral_centroid, spectral_bandwidth])\n",
        "            except Exception as e:\n",
        "                feature_vector.extend([0, 0])\n",
        "\n",
        "            # F0 Statistics (Prosodic Features)\n",
        "            f0_values = []\n",
        "            try:\n",
        "                pitch = sound.to_pitch()\n",
        "                f0_values = pitch.selected_array['frequency']\n",
        "                f0_values = f0_values[f0_values != 0] # Exclude unvoiced frames (0 Hz)\n",
        "                if len(f0_values) > 0:\n",
        "                    f0_mean = np.mean(f0_values)\n",
        "                    f0_std = np.std(f0_values)\n",
        "                    f0_range = np.max(f0_values) - np.min(f0_values)\n",
        "                else:\n",
        "                    f0_mean, f0_std, f0_range = 0, 0, 0 # Handle case with no voiced frames\n",
        "\n",
        "                feature_vector.extend([f0_mean, f0_std, f0_range])\n",
        "            except Exception as e:\n",
        "                feature_vector.extend([0, 0, 0])\n",
        "\n",
        "            # Nonlinear Dynamics (RPDE, D2, DFA)\n",
        "            rpde_val, d2_val, dfa_val = 0, 0, 0 # Default to 0 if not computed\n",
        "\n",
        "            if len(f0_values) > 10: # Need enough points for nolds functions\n",
        "                try:\n",
        "                    # These can be computationally expensive, consider skipping for large files\n",
        "                    if len(f0_values) < 5000:  # Limit to prevent memory issues\n",
        "                        rpde_val = nolds.rpde(f0_values)\n",
        "                        d2_val = nolds.d2(f0_values)\n",
        "                        dfa_val = nolds.dfa(f0_values)\n",
        "                except Exception as e:\n",
        "                    pass # Keep default 0 values\n",
        "\n",
        "            feature_vector.extend([rpde_val, d2_val, dfa_val])\n",
        "\n",
        "            # Check if feature vector has consistent length\n",
        "            expected_length = 29 # Update if adding/removing features\n",
        "            if len(feature_vector) == expected_length:\n",
        "                features.append(feature_vector)\n",
        "                labels.append(label)\n",
        "                ids.append(patient_id)\n",
        "                times.append(time_before_diagnosis)\n",
        "                lengths.append(duration)  # Using duration instead of audio_length\n",
        "                successful_paths.append(audio_path)\n",
        "            else:\n",
        "                print(f\"Skipping {audio_path} due to inconsistent feature vector length ({len(feature_vector)} != {expected_length})\")\n",
        "\n",
        "            # Force garbage collection to free memory\n",
        "            gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred processing file: {audio_path}. Error: {e}\")\n",
        "\n",
        "    features_array = np.array(features, dtype=np.float32)\n",
        "    return features_array, labels, ids, times, lengths, successful_paths  # Return all data, not just features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cllmJlq4ITS-"
      },
      "source": [
        "# Loading CSV helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "m-2OzAVgITsK"
      },
      "outputs": [],
      "source": [
        "# New function to load features from CSV\n",
        "def load_features_from_csv(filename):\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"File not found: {filename}\")\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "    print(f\"Loading features from: {filename}\")\n",
        "    df = pd.read_csv(filename)\n",
        "    required_columns = {'label', 'id', 'time_before_diagnosis', 'length'}\n",
        "    if not required_columns.issubset(df.columns):\n",
        "        print(f\"Error: One or more required columns {required_columns} missing in {filename}\")\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "    labels = df['label'].values.astype(int)\n",
        "    speaker_ids = df['id'].values\n",
        "    times = df['time_before_diagnosis'].values\n",
        "    lengths = df['length'].values\n",
        "\n",
        "    features = df.drop(columns=['label', 'id', 'time_before_diagnosis', 'length']).values.astype(np.float32)\n",
        "    features = np.nan_to_num(features)\n",
        "\n",
        "    return features, labels, speaker_ids, times, lengths, df\n",
        "\n",
        "\n",
        "\n",
        "# Save features function\n",
        "def save_features(features, labels, ids, time_before_diagnosis, audio_length, filename):\n",
        "    \"\"\"Save features and labels to a CSV file.\"\"\"\n",
        "    if features.size == 0 or not labels:\n",
        "        print(f\"Warning: No features or labels to save for {filename}\")\n",
        "        return\n",
        "    df = pd.DataFrame(features)\n",
        "    df['label'] = labels\n",
        "    df['id'] = ids\n",
        "    df['time_before_diagnosis'] = time_before_diagnosis\n",
        "    df['length'] = audio_length\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Saved features to {filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lxwzcv0hIXU1"
      },
      "source": [
        "# Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DznagIoBIYoQ"
      },
      "outputs": [],
      "source": [
        "class DeepNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(DeepNN, self).__init__()\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_dim, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.3),\n",
        "            torch.nn.Linear(256, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.3),\n",
        "            torch.nn.Linear(128, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, 1),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class ImprovedDeepNN(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes=1, dropout=0.3):\n",
        "        super(ImprovedDeepNN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.bn4 = nn.BatchNorm1d(64)\n",
        "\n",
        "        self.output = nn.Linear(64, num_classes)  # 1 for binary, >1 for multi-class (depends on number of bins)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = F.relu(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = F.relu(self.bn4(self.fc4(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        if self.num_classes == 1:\n",
        "            return torch.sigmoid(self.output(x))  # Binary classification\n",
        "        else:\n",
        "            return self.output(x)  # Use CrossEntropyLoss with raw logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83TD4k9YaXrv"
      },
      "source": [
        "# Model Training/Testing/Comparison Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ATb3Un62aX7p"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(y_true, y_pred_prob):\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"roc_auc\": roc_auc_score(y_true, y_pred_prob) if len(np.unique(y_true)) > 1 else np.nan,\n",
        "        \"sensitivity\": recall_score(y_true, y_pred),\n",
        "        \"specificity\": recall_score(1 - y_true, 1 - y_pred),\n",
        "        \"f1_score\": f1_score(y_true, y_pred),\n",
        "        \"confusion_matrix\": confusion_matrix(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "def average_metrics(results_list):\n",
        "    return {\n",
        "        key: np.nanmean([res[key] for res in results_list])\n",
        "        for key in results_list[0].keys()\n",
        "    }\n",
        "\n",
        "def empty_metrics():\n",
        "    return {\n",
        "        \"accuracy\": np.nan,\n",
        "        \"roc_auc\": np.nan,\n",
        "        \"sensitivity\": np.nan,\n",
        "        \"specificity\": np.nan,\n",
        "        \"f1_score\": np.nan,\n",
        "        \"confusion_matrix\": np.array([[0, 0], [0, 0]])\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pZtXFUrIh9O"
      },
      "source": [
        "# Training and testing DNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5Kmd_vs_IiQL"
      },
      "outputs": [],
      "source": [
        "def train_evaluate_model(X_train, y_train, X_test, y_test, feature_type, model_to_use):\n",
        "    \"\"\"Train and evaluate model for specific feature type\"\"\"\n",
        "    if X_train.size == 0 or X_test.size == 0:\n",
        "        print(f\"Warning: Empty feature sets for {feature_type}. Skipping evaluation.\")\n",
        "        return empty_metrics()\n",
        "\n",
        "    # For neural network models\n",
        "    if not model_to_use.__name__.startswith('RandomForestClassifier'):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Create model with appropriate input dimensions\n",
        "        input_dim = X_train.shape[1]\n",
        "        model = model_to_use(input_dim).to(device)\n",
        "\n",
        "        # Rest of training logic remains the same\n",
        "        criterion = torch.nn.BCELoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        # Convert data to tensors\n",
        "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1).to(device)\n",
        "\n",
        "        # Training loop\n",
        "        dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n",
        "\n",
        "        for epoch in range(100):\n",
        "            model.train()\n",
        "            for batch_X, batch_y in dataloader:\n",
        "                outputs = model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_pred_prob = model(X_test_tensor).cpu().numpy()\n",
        "\n",
        "    # For Random Forest\n",
        "    elif model_to_use.__name__ == 'RandomForestClassifier':\n",
        "        # Create and train the random forest model\n",
        "        model = model_to_use(\n",
        "            n_estimators=100,\n",
        "            max_depth=None,\n",
        "            min_samples_split=2,\n",
        "            min_samples_leaf=1,\n",
        "            max_features='sqrt',\n",
        "            bootstrap=True,\n",
        "            random_state=37,\n",
        "            n_jobs=-1,\n",
        "            class_weight='balanced'\n",
        "        )\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Get probability predictions\n",
        "        y_pred_prob = model.predict_proba(X_test)[:, 1].reshape(-1, 1)\n",
        "\n",
        "    # For other sklearn models\n",
        "    else:\n",
        "        model = model_to_use()\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred_prob = model.predict_proba(X_test)[:, 1].reshape(-1, 1)\n",
        "\n",
        "    return calculate_metrics(y_test, y_pred_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhMR2mh_Ixx8"
      },
      "source": [
        "# Train model + Feature comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BywVXbGnGpTb"
      },
      "outputs": [],
      "source": [
        "def crossval_compare_features(dataset_path, k=5, save_figures=True, min_audio_length=0, deep_features=\"Whisper Small\" ,model=None):\n",
        "    # Determine which model to use\n",
        "    if model == \"random_forest\":\n",
        "        model_to_use = RandomForestClassifier\n",
        "        model_name = \"RandomForest\"\n",
        "    elif isinstance(model, type) or callable(model):\n",
        "        # If model is a class or function\n",
        "        model_to_use = model\n",
        "        model_name = model.__name__\n",
        "    else:\n",
        "        # If model is an instance\n",
        "        model_to_use = model\n",
        "        model_name = model.__class__.__name__\n",
        "\n",
        "    save_folder = f'visualization_results_{model_name}'\n",
        "    # Create directory for saving figures if needed\n",
        "    os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "    # Load data with proper alignment\n",
        "    all_audio_paths, all_labels, all_speaker_ids = load_parkceleb_dataset_with_speakers(dataset_path)\n",
        "\n",
        "    # Load features\n",
        "    if deep_features == \"Whisper Small\":\n",
        "        whisper_data = load_features_from_csv(\"whisper_small_all_features.csv\")\n",
        "        traditional_data = load_features_from_csv(\"traditional_all_features.csv\")\n",
        "    elif deep_features == \"Whisper Medium\":\n",
        "        whisper_data = load_features_from_csv(\"whisper_medium_all_features.csv\")\n",
        "        traditional_data = load_features_from_csv(\"traditional_all_features.csv\")\n",
        "\n",
        "\n",
        "    if whisper_data[0] is None:\n",
        "        print(\"Whisper data is None. Generating features.\")\n",
        "        features, labels, ids, times, lengths, _ = extract_whisper_features(all_audio_paths)\n",
        "        # Change CSV names according to whisper model used\n",
        "        save_features(features, labels, ids, times, lengths, \"whisper_medium_all_features.csv\")\n",
        "        whisper_data = load_features_from_csv(\"whisper_medium_all_features.csv\")\n",
        "\n",
        "\n",
        "    if traditional_data[0] is None:\n",
        "        print(\"Traditional data is None. Generating features.\")\n",
        "        features, labels, ids, times, lengths, _ = extract_traditional_features(dataset_path)\n",
        "        save_features(features, labels, ids, times, lengths, \"traditional_all_features.csv\")\n",
        "        traditional_data = load_features_from_csv(\"traditional_all_features.csv\")\n",
        "\n",
        "    # Dictionary to store all results\n",
        "    results = {}\n",
        "    all_fold_metrics = {}  # Store metrics for each fold for later visualization\n",
        "\n",
        "    for feature_type, (features, labels, speakers, times, lengths, _) in [\n",
        "        (\"Whisper\", whisper_data),\n",
        "        (\"Traditional\", traditional_data)\n",
        "    ]:\n",
        "        if features is None:\n",
        "            continue\n",
        "\n",
        "        length_mask = lengths >= min_audio_length\n",
        "        features = features[length_mask]\n",
        "        labels = labels[length_mask]\n",
        "        speakers = speakers[length_mask]\n",
        "        times = times[length_mask]\n",
        "        lengths = lengths[length_mask]\n",
        "\n",
        "        print(f\"\\n=== Processing {feature_type} Features with {model_name} ===\")\n",
        "        print(f\"Feature dimension: {features.shape[1]}\")\n",
        "        print(f\"Class distribution: {np.bincount(labels)}\")\n",
        "\n",
        "        # Speaker-level stratified K-Fold\n",
        "        unique_speakers = np.unique(speakers)\n",
        "        speaker_labels = np.array([1 if 'pd' in s else 0 for s in unique_speakers])\n",
        "\n",
        "        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=37)\n",
        "        fold_results = []\n",
        "        fold_metrics_dict = {\n",
        "            'accuracy': [], 'roc_auc': [], 'sensitivity': [],\n",
        "            'specificity': [], 'f1_score': [], 'confusion_matrices': [],\n",
        "            'y_test': [], 'y_pred_prob': [], 'feature_importance': []  # Added feature importance\n",
        "        }\n",
        "\n",
        "        for fold, (train_idx, test_idx) in enumerate(skf.split(unique_speakers, speaker_labels)):\n",
        "            print(f\"\\n=== Fold {fold+1} ===\")\n",
        "            train_speakers = set(unique_speakers[train_idx])\n",
        "            test_speakers = set(unique_speakers[test_idx])\n",
        "\n",
        "            # Select data for current fold\n",
        "            train_mask = np.isin(speakers, list(train_speakers))\n",
        "            test_mask = np.isin(speakers, list(test_speakers))\n",
        "\n",
        "            X_train, X_test = features[train_mask], features[test_mask]\n",
        "            y_train, y_test = labels[train_mask], labels[test_mask]\n",
        "\n",
        "            # Check for valid class distribution\n",
        "            if len(np.unique(y_test)) < 2:\n",
        "                print(f\"Skipping fold {fold+1} - single class in test set\")\n",
        "                continue\n",
        "\n",
        "            # Standardize features\n",
        "            scaler = StandardScaler()\n",
        "            X_train = scaler.fit_transform(X_train)\n",
        "            X_test = scaler.transform(X_test)\n",
        "\n",
        "            # Train and Eval\n",
        "            metrics = train_evaluate_model(X_train, y_train, X_test, y_test, feature_type, model)\n",
        "            fold_results.append(metrics)\n",
        "\n",
        "            # Extract y_pred_prob for ROC curve generation (we'll need to regenerate it)\n",
        "            y_pred = (metrics['confusion_matrix'][:, 1].sum(), metrics['confusion_matrix'][:, 0].sum())\n",
        "\n",
        "            # Store detailed metrics for visualization\n",
        "            for key in ['accuracy', 'roc_auc', 'sensitivity', 'specificity', 'f1_score']:\n",
        "                fold_metrics_dict[key].append(metrics[key])\n",
        "            fold_metrics_dict['confusion_matrices'].append(metrics['confusion_matrix'])\n",
        "            fold_metrics_dict['y_test'].append(y_test)\n",
        "\n",
        "            # Visualize confusion matrix for this fold\n",
        "            plt.figure(figsize=(6, 5))\n",
        "            cm = metrics['confusion_matrix']\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                        xticklabels=['Control', 'PD'], yticklabels=['Control', 'PD'])\n",
        "            plt.xlabel('Predicted')\n",
        "            plt.ylabel('True')\n",
        "            plt.title(f'{feature_type} - Fold {fold+1} Confusion Matrix')\n",
        "            if save_figures:\n",
        "                plt.savefig(f'{save_folder}/{feature_type}_fold{fold+1}_confusion.png', bbox_inches='tight')\n",
        "                plt.close()\n",
        "            else:\n",
        "                plt.show()\n",
        "\n",
        "        # Store results\n",
        "        if fold_results:\n",
        "            results[feature_type] = average_metrics(fold_results)\n",
        "            all_fold_metrics[feature_type] = fold_metrics_dict\n",
        "\n",
        "            # Visualize metrics across folds\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            metrics_across_folds = pd.DataFrame({\n",
        "                'Accuracy': fold_metrics_dict['accuracy'],\n",
        "                'AUC': fold_metrics_dict['roc_auc'],\n",
        "                'Sensitivity': fold_metrics_dict['sensitivity'],\n",
        "                'Specificity': fold_metrics_dict['specificity'],\n",
        "                'F1 Score': fold_metrics_dict['f1_score']\n",
        "            })\n",
        "\n",
        "            metrics_across_folds.index = [f'Fold {i+1}' for i in range(len(metrics_across_folds))]\n",
        "            ax = metrics_across_folds.plot(kind='bar', figsize=(10, 6))\n",
        "            plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.3)  # baseline\n",
        "            plt.title(f'{feature_type} - Performance Metrics Across Folds')\n",
        "            plt.ylabel('Score')\n",
        "            plt.ylim(0, 1.05)\n",
        "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "            plt.legend(loc='lower center', bbox_to_anchor=(0.5, -0.25), ncol=5)\n",
        "\n",
        "            # Add value labels on bars\n",
        "            for container in ax.containers:\n",
        "                ax.bar_label(container, fmt='%.2f', fontsize=8)\n",
        "\n",
        "            if save_figures:\n",
        "                plt.savefig(f'{save_folder}/{feature_type}_metrics_by_fold.png', bbox_inches='tight')\n",
        "                plt.close()\n",
        "            else:\n",
        "                plt.show()\n",
        "\n",
        "            # Visualize average confusion matrix\n",
        "            plt.figure(figsize=(6, 5))\n",
        "            avg_cm = np.mean([cm for cm in fold_metrics_dict['confusion_matrices']], axis=0)\n",
        "            sns.heatmap(avg_cm, annot=True, fmt='.1f', cmap='Blues',\n",
        "                        xticklabels=['Control', 'PD'], yticklabels=['Control', 'PD'])\n",
        "            plt.xlabel('Predicted')\n",
        "            plt.ylabel('True')\n",
        "            plt.title(f'{feature_type} - Average Confusion Matrix')\n",
        "            if save_figures:\n",
        "                plt.savefig(f'{save_folder}/{feature_type}_avg_confusion.png', bbox_inches='tight')\n",
        "                plt.close()\n",
        "            else:\n",
        "                plt.show()\n",
        "\n",
        "\n",
        "        else:\n",
        "            results[feature_type] = empty_metrics()\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n=== Final Results ===\")\n",
        "    for feature_type, metrics in results.items():\n",
        "        print(f\"\\n{feature_type} Features:\")\n",
        "        for metric, value in metrics.items():\n",
        "            if metric != 'confusion_matrix':\n",
        "                print(f\"{metric:15}: {value:.4f}\")\n",
        "\n",
        "    # Compare feature types if both are available\n",
        "    if len(results) > 1:\n",
        "        # Create comparison bar chart\n",
        "        metric_names = ['Accuracy', 'AUC', 'Sensitivity', 'Specificity', 'F1 Score']\n",
        "        feature_types = list(results.keys())\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        width = 0.35\n",
        "        x = np.arange(len(metric_names))\n",
        "\n",
        "        for i, feature_type in enumerate(feature_types):\n",
        "            metrics_values = [\n",
        "                results[feature_type]['accuracy'],\n",
        "                results[feature_type]['roc_auc'],\n",
        "                results[feature_type]['sensitivity'],\n",
        "                results[feature_type]['specificity'],\n",
        "                results[feature_type]['f1_score']\n",
        "            ]\n",
        "            plt.bar(x + (i - 0.5*(len(feature_types)-1)) * width, metrics_values,\n",
        "                   width, label=feature_type)\n",
        "\n",
        "            # Add value labels on bars\n",
        "            for j, value in enumerate(metrics_values):\n",
        "                plt.text(x[j] + (i - 0.5*(len(feature_types)-1)) * width, value + 0.01,\n",
        "                         f'{value:.3f}', ha='center', fontsize=8)\n",
        "\n",
        "        plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.3)  # baseline\n",
        "        plt.xlabel('Metric')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('Performance Comparison Between Feature Types')\n",
        "        plt.xticks(x, metric_names)\n",
        "        plt.ylim(0, 1.05)\n",
        "        plt.legend()\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "        if save_figures:\n",
        "            plt.savefig(f'{save_folder}/feature_comparison.png', bbox_inches='tight')\n",
        "            plt.close()\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "WU1MTjz2P0rL",
        "outputId": "9e8fec3e-3497-4c69-fb80-18f7a854acb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading features from: whisper_small_all_features.csv\n",
            "Loading features from: traditional_all_features.csv\n",
            "\n",
            "=== Processing Whisper Features with DeepNN ===\n",
            "Feature dimension: 768\n",
            "Class distribution: [5239 8614]\n",
            "\n",
            "=== Fold 1 ===\n",
            "\n",
            "=== Fold 2 ===\n",
            "\n",
            "=== Fold 3 ===\n",
            "\n",
            "=== Fold 4 ===\n",
            "\n",
            "=== Fold 5 ===\n",
            "\n",
            "=== Processing Traditional Features with DeepNN ===\n",
            "Feature dimension: 29\n",
            "Class distribution: [5239 8614]\n",
            "\n",
            "=== Fold 1 ===\n",
            "\n",
            "=== Fold 2 ===\n",
            "\n",
            "=== Fold 3 ===\n",
            "\n",
            "=== Fold 4 ===\n",
            "\n",
            "=== Fold 5 ===\n",
            "\n",
            "=== Final Results ===\n",
            "\n",
            "Whisper Features:\n",
            "accuracy       : 0.5445\n",
            "roc_auc        : 0.5582\n",
            "sensitivity    : 0.5504\n",
            "specificity    : 0.5279\n",
            "f1_score       : 0.5948\n",
            "\n",
            "Traditional Features:\n",
            "accuracy       : 0.5547\n",
            "roc_auc        : 0.5735\n",
            "sensitivity    : 0.5370\n",
            "specificity    : 0.6020\n",
            "f1_score       : 0.5954\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "DeepNN_small_results = crossval_compare_features(\"parkceleb_filtered\", min_audio_length=0.1, deep_features=\"Whisper Small\" ,model=DeepNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "f23TWKETHfkp",
        "outputId": "85180a6a-9a2b-4557-8806-a9188d6c1f54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading features from: whisper_small_all_features.csv\n",
            "Loading features from: traditional_all_features.csv\n",
            "\n",
            "=== Processing Whisper Features with ImprovedDeepNN ===\n",
            "Feature dimension: 768\n",
            "Class distribution: [5239 8614]\n",
            "\n",
            "=== Fold 1 ===\n",
            "\n",
            "=== Fold 2 ===\n",
            "\n",
            "=== Fold 3 ===\n",
            "\n",
            "=== Fold 4 ===\n",
            "\n",
            "=== Fold 5 ===\n",
            "\n",
            "=== Processing Traditional Features with ImprovedDeepNN ===\n",
            "Feature dimension: 29\n",
            "Class distribution: [5239 8614]\n",
            "\n",
            "=== Fold 1 ===\n",
            "\n",
            "=== Fold 2 ===\n",
            "\n",
            "=== Fold 3 ===\n",
            "\n",
            "=== Fold 4 ===\n",
            "\n",
            "=== Fold 5 ===\n",
            "\n",
            "=== Final Results ===\n",
            "\n",
            "Whisper Features:\n",
            "accuracy       : 0.5282\n",
            "roc_auc        : 0.5490\n",
            "sensitivity    : 0.5180\n",
            "specificity    : 0.5405\n",
            "f1_score       : 0.5725\n",
            "\n",
            "Traditional Features:\n",
            "accuracy       : 0.5635\n",
            "roc_auc        : 0.5913\n",
            "sensitivity    : 0.5703\n",
            "specificity    : 0.5752\n",
            "f1_score       : 0.6146\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "ImprovedDeepNN_small_results = crossval_compare_features(\"parkceleb_filtered\", min_audio_length=0.1, deep_features=\"Whisper Small\", model=ImprovedDeepNN)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RF_small_results = crossval_compare_features(\"parkceleb_filtered\", min_audio_length=0.1, deep_features=\"Whisper Small\", model=RandomForestClassifier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "f5QZyexnMvG8",
        "outputId": "1f2bf932-8b1a-42ce-eefc-1cb52bc913c7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading features from: whisper_small_all_features.csv\n",
            "Loading features from: traditional_all_features.csv\n",
            "\n",
            "=== Processing Whisper Features with RandomForestClassifier ===\n",
            "Feature dimension: 768\n",
            "Class distribution: [5239 8614]\n",
            "\n",
            "=== Fold 1 ===\n",
            "\n",
            "=== Fold 2 ===\n",
            "\n",
            "=== Fold 3 ===\n",
            "\n",
            "=== Fold 4 ===\n",
            "\n",
            "=== Fold 5 ===\n",
            "\n",
            "=== Processing Traditional Features with RandomForestClassifier ===\n",
            "Feature dimension: 29\n",
            "Class distribution: [5239 8614]\n",
            "\n",
            "=== Fold 1 ===\n",
            "\n",
            "=== Fold 2 ===\n",
            "\n",
            "=== Fold 3 ===\n",
            "\n",
            "=== Fold 4 ===\n",
            "\n",
            "=== Fold 5 ===\n",
            "\n",
            "=== Final Results ===\n",
            "\n",
            "Whisper Features:\n",
            "accuracy       : 0.6056\n",
            "roc_auc        : 0.5710\n",
            "sensitivity    : 0.7975\n",
            "specificity    : 0.3222\n",
            "f1_score       : 0.7094\n",
            "\n",
            "Traditional Features:\n",
            "accuracy       : 0.5909\n",
            "roc_auc        : 0.6060\n",
            "sensitivity    : 0.7080\n",
            "specificity    : 0.4384\n",
            "f1_score       : 0.6777\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DeepNN_med_results = crossval_compare_features(\"parkceleb_filtered\", min_audio_length=0.1, deep_features=\"Whisper Medium\" ,model=DeepNN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "ibNO0alhNATE",
        "outputId": "fa981ab0-ad17-4e40-bc96-ad2b7df997f4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading features from: whisper_medium_all_features.csv\n",
            "Loading features from: traditional_all_features.csv\n",
            "\n",
            "=== Processing Whisper Features with DeepNN ===\n",
            "Feature dimension: 1024\n",
            "Class distribution: [5239 8614]\n",
            "\n",
            "=== Fold 1 ===\n",
            "\n",
            "=== Fold 2 ===\n",
            "\n",
            "=== Fold 3 ===\n",
            "\n",
            "=== Fold 4 ===\n",
            "\n",
            "=== Fold 5 ===\n",
            "\n",
            "=== Processing Traditional Features with DeepNN ===\n",
            "Feature dimension: 29\n",
            "Class distribution: [5239 8614]\n",
            "\n",
            "=== Fold 1 ===\n",
            "\n",
            "=== Fold 2 ===\n",
            "\n",
            "=== Fold 3 ===\n",
            "\n",
            "=== Fold 4 ===\n",
            "\n",
            "=== Fold 5 ===\n",
            "\n",
            "=== Final Results ===\n",
            "\n",
            "Whisper Features:\n",
            "accuracy       : 0.5509\n",
            "roc_auc        : 0.5661\n",
            "sensitivity    : 0.5745\n",
            "specificity    : 0.5179\n",
            "f1_score       : 0.6069\n",
            "\n",
            "Traditional Features:\n",
            "accuracy       : 0.5413\n",
            "roc_auc        : 0.5763\n",
            "sensitivity    : 0.5271\n",
            "specificity    : 0.5809\n",
            "f1_score       : 0.5805\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ImprovedDeepNN_med_results = crossval_compare_features(\"parkceleb_filtered\", min_audio_length=0.1, deep_features=\"Whisper Medium\", model=ImprovedDeepNN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "UUz6DeasNC2K",
        "outputId": "593ef837-44e8-4b18-c8cf-8723babe207d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading features from: whisper_medium_all_features.csv\n",
            "Loading features from: traditional_all_features.csv\n",
            "\n",
            "=== Processing Whisper Features with ImprovedDeepNN ===\n",
            "Feature dimension: 1024\n",
            "Class distribution: [5239 8614]\n",
            "\n",
            "=== Fold 1 ===\n",
            "\n",
            "=== Fold 2 ===\n",
            "\n",
            "=== Fold 3 ===\n",
            "\n",
            "=== Fold 4 ===\n",
            "\n",
            "=== Fold 5 ===\n",
            "\n",
            "=== Processing Traditional Features with ImprovedDeepNN ===\n",
            "Feature dimension: 29\n",
            "Class distribution: [5239 8614]\n",
            "\n",
            "=== Fold 1 ===\n",
            "\n",
            "=== Fold 2 ===\n",
            "\n",
            "=== Fold 3 ===\n",
            "\n",
            "=== Fold 4 ===\n",
            "\n",
            "=== Fold 5 ===\n",
            "\n",
            "=== Final Results ===\n",
            "\n",
            "Whisper Features:\n",
            "accuracy       : 0.5668\n",
            "roc_auc        : 0.5785\n",
            "sensitivity    : 0.5785\n",
            "specificity    : 0.5367\n",
            "f1_score       : 0.6208\n",
            "\n",
            "Traditional Features:\n",
            "accuracy       : 0.5290\n",
            "roc_auc        : 0.5663\n",
            "sensitivity    : 0.5299\n",
            "specificity    : 0.5500\n",
            "f1_score       : 0.5754\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RF_med_results = crossval_compare_features(\"parkceleb_filtered\", min_audio_length=0.1, deep_features=\"Whisper Medium\", model=RandomForestClassifier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "TpDnOof8NExJ",
        "outputId": "20bf778d-8dfe-43f5-eccf-c641e2c989d4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading features from: whisper_medium_all_features.csv\n",
            "Loading features from: traditional_all_features.csv\n",
            "\n",
            "=== Processing Whisper Features with RandomForestClassifier ===\n",
            "Feature dimension: 1024\n",
            "Class distribution: [5239 8614]\n",
            "\n",
            "=== Fold 1 ===\n",
            "\n",
            "=== Fold 2 ===\n",
            "\n",
            "=== Fold 3 ===\n",
            "\n",
            "=== Fold 4 ===\n",
            "\n",
            "=== Fold 5 ===\n",
            "\n",
            "=== Processing Traditional Features with RandomForestClassifier ===\n",
            "Feature dimension: 29\n",
            "Class distribution: [5239 8614]\n",
            "\n",
            "=== Fold 1 ===\n",
            "\n",
            "=== Fold 2 ===\n",
            "\n",
            "=== Fold 3 ===\n",
            "\n",
            "=== Fold 4 ===\n",
            "\n",
            "=== Fold 5 ===\n",
            "\n",
            "=== Final Results ===\n",
            "\n",
            "Whisper Features:\n",
            "accuracy       : 0.6094\n",
            "roc_auc        : 0.6003\n",
            "sensitivity    : 0.7973\n",
            "specificity    : 0.3372\n",
            "f1_score       : 0.7108\n",
            "\n",
            "Traditional Features:\n",
            "accuracy       : 0.5909\n",
            "roc_auc        : 0.6060\n",
            "sensitivity    : 0.7080\n",
            "specificity    : 0.4384\n",
            "f1_score       : 0.6777\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtenXgkUM34r"
      },
      "source": [
        "# Sanity checks\n",
        "I messed up the save to csv part of whisper feature generation, so I had to fix it. That's what the code below is for. The function is also fixed now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV2aBcE7M53w"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"whisper_small_all_features.csv\")\n",
        "print(\"Unique speaker IDs:\", df['id'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmc4pCkrM8W8"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "shutil.copy2(\"whisper_all_features.csv\", \"whisper_all_features_BACKUP.csv\")\n",
        "shutil.copy2(\"traditional_all_features.csv\", \"traditional_all_features_BACKUP.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieh1zB8gNE8T"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"whisper_all_features.csv\")\n",
        "print(\"PD count:\", df[df['label'] == 1].shape[0])\n",
        "print(\"CN count:\", df[df['label'] == 0].shape[0])\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"traditional_all_features.csv\")\n",
        "print(\"PD count:\", df[df['label'] == 1].shape[0])\n",
        "print(\"CN count:\", df[df['label'] == 0].shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpo2UKDDM3tk"
      },
      "outputs": [],
      "source": [
        "def fix_labels_in_csv(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Create labels based on 'id' column\n",
        "    df['label'] = df['id'].str.contains('pd', case=False).astype(int)\n",
        "\n",
        "    # Save fixed CSV (backup original first!)\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"Updated labels in {csv_path}. Class distribution: {df['label'].value_counts().to_dict()}\")\n",
        "\n",
        "# Apply to both feature CSVs\n",
        "fix_labels_in_csv(\"whisper_medium_all_features.csv\")\n",
        "# fix_labels_in_csv(\"traditional_all_features.csv\")\n",
        "\n",
        "df = pd.read_csv(\"whisper_medium_all_features.csv\")\n",
        "print(\"PD count:\", df[df['label'] == 1].shape[0])\n",
        "print(\"CN count:\", df[df['label'] == 0].shape[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
